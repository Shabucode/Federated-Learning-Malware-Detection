#!/usr/bin/env python
# coding: utf-8

# In[ ]:


get_ipython().system('pip install -q flwr[simulation] torch torchvision matplotlib')
get_ipython().system('pip install scikit-learn')


# In[ ]:


get_ipython().run_cell_magic('bash', '', '\npip3 install -q gdown seaborn pandas plotly scikit-learn ipywidgets torch\n\npip3 install -q nbformat>=4.2.0')


# In[ ]:


from collections import OrderedDict
from typing import List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from sklearn.model_selection import train_test_split
import csv
import torch.optim as optim


import flwr as fl
from flwr.common import Metrics

DEVICE = torch.device("cpu")  # Try "cuda" to train on GPU
print(
    f"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}"
)


# In[ ]:


from google.colab import drive
drive.mount('/content/drive')


# In[ ]:


data = pd.read_csv('/content/drive/MyDrive/N-BaIoT Dataset.csv')
print(data.head())


# In[ ]:


dataadv = pd.read_csv('/content/drive/MyDrive/adversarial_samples (3).csv')
print(dataadv.head())


# In[ ]:


oridata = pd.read_csv('/content/drive/MyDrive/N-BaIoT Dataset.csv')


# In[ ]:


# Define the new column names
new_column_names = [
    "Flags", "Protocol", "Source Address", "Source port", "Direction",
    "Destination Address", "Destination port", "Total Packets", "Total Bytes",
    "State", "Labels"
]

# Rename the columns
dataadv.columns = new_column_names

print(dataadv.head())


# In[ ]:


import seaborn as sns
import plotly.express as px


# In[ ]:


# Check shape of the data
print(f"Shape of the data: {data.shape}")


# In[ ]:


# Check shape of the data
print(f"Shape of the data: {dataadv.shape}")


# In[ ]:


data.describe()


# In[ ]:


dataadv.describe()


# In[ ]:


data.dtypes.value_counts


# In[ ]:


dataadv.dtypes.value_counts


# In[ ]:


MissingValues = data.isnull().sum().sum()
print(f" Number of Missing Values : {MissingValues}")


# In[ ]:


MissingValues1 = dataadv.isnull().sum().sum()
print(f" Number of Missing Values : {MissingValues}")


# In[ ]:


DuplicatedValues = data.duplicated().sum()
print(f"Number of duplicated values : {DuplicatedValues}")


# In[ ]:


DuplicatedValuesadv = dataadv.duplicated().sum()
print(f"Number of duplicated values : {DuplicatedValuesadv}")


# In[ ]:


data = data.drop_duplicates(keep = "first")


# In[ ]:


dataadv = dataadv.drop_duplicates(keep = "first")


# In[ ]:


#check the shape of the data after removing dublicates
print(f"Shape of the data: {data.shape}")


# In[ ]:


#check the shape of the data after removing dublicates
print(f"Shape of the data: {dataadv.shape}")


# In[ ]:


# check for data imbalance in Labels column
counts = data["Labels"].value_counts()

print(f"Counts for each class:\n{counts}")


# In[ ]:


# check for data imbalance in Labels column
countsadv = dataadv["Labels"].value_counts()

print(f"Counts for each class in adversarial samples:\n{countsadv}")


# Exploratory Analysis
# 

# Correlation Matrix

# In[ ]:


# Calculate the correlation matrix
correlation_matrix = data.corr()

# Display the correlation matrix
fig = px.imshow(correlation_matrix, color_continuous_scale="Greens")

# Update the layout to make the plot full screen and hide the toolbar
fig.update_layout(
    title="Correlation Matrix",
    autosize=False,
    width=1200,
    height=1140,
    margin=dict(l=0, r=0, t=40, b=0),  # Adjust the margins as needed
    showlegend=False,
    coloraxis_colorbar=dict(thickness=10),
)

# Hide the Plotly mode bar and display the plot
fig.show(config={"displayModeBar": False})


# In[ ]:


# Calculate the correlation matrix
correlation_matrixadv = dataadv.corr()

# Display the correlation matrix
figadv = px.imshow(correlation_matrixadv, color_continuous_scale="reds")

# Update the layout to make the plot full screen and hide the toolbar
figadv.update_layout(
    title="Correlation Matrixadv",
    autosize=False,
    width=1200,
    height=1140,
    margin=dict(l=0, r=0, t=40, b=0),  # Adjust the margins as needed
    showlegend=False,
    coloraxis_colorbar=dict(thickness=10),
)

# Hide the Plotly mode bar and display the plot
figadv.show(config={"displayModeBar": False})


# Prepare the Dataset

# In[ ]:


X = data.drop(columns=["Labels"])
y = data["Labels"]


# In[ ]:


Xadv = dataadv.drop(columns=["Labels"])
yadv = dataadv["Labels"]


# PCA

# In[ ]:


from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Create a StandardScaler instance
scaler = StandardScaler()

# standardize the features
X = scaler.fit_transform(X)


# In[ ]:


from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Create a StandardScaler instance
scaleradv = StandardScaler()

# standardize the features
Xadv = scaleradv.fit_transform(Xadv)


# Split the dataset

# In[ ]:


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=42, test_size=0.2, stratify=y
)

# print the shape of X_train and X_test
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")


# In[ ]:


print("Shape of Xadv:", Xadv.shape)
print("Shape of yadv:", yadv.shape)


# In[ ]:


from sklearn.model_selection import train_test_split

Xadv_train, Xadv_test, yadv_train, yadv_test = train_test_split(
    Xadv, yadv, random_state=42, test_size=0.2, stratify=yadv
)

# print the shape of X_train and X_test
print(f"Shape of X_train: {Xadv_train.shape}")
print(f"Shape of X_test: {Xadv_test.shape}")


# In[ ]:


X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# In[ ]:


Xadv_train = scaleradv.fit_transform(Xadv_train)
Xadv_test = scaleradv.transform(Xadv_test)


# Partition Dataset

# In[ ]:


# partition the training data into 10 parts
X_train_store = np.array_split(X_train, 10)
y_train_store = np.array_split(y_train, 10)

# partition the test data into 10 parts
X_test_store = np.array_split(X_test, 10)
y_test_store = np.array_split(y_test, 10)


# In[ ]:


# partition the training data into 10 parts
Xadv_train_store = np.array_split(Xadv_train, 10)
yadv_train_store = np.array_split(yadv_train, 10)

# partition the test data into 10 parts
Xadv_test_store = np.array_split(Xadv_test, 10)
yadv_test_store = np.array_split(yadv_test, 10)


# Models

# In[ ]:


import flwr as fl
from flwr.common import Metrics

# Check flwr version
print(f"FLWR version {fl.__version__}")


# Configuration

# In[ ]:


# Define classes
classes = [0, 1]
# Define no. of Orgs
num_clients = 10


# Aggregate Accuracy

# In[ ]:


# Modify the weighted_average function
def weighted_average(weights):
    """Returns the weighted average of the given weights."""
    accuracy_sum = 0
    precision_sum = 0
    recall_sum = 0
    f1_sum = 0
    total_examples = 0

    for num_examples, metrics in weights:
        accuracy_sum += num_examples * metrics["accuracy"]
        precision_sum += num_examples * metrics["precision"]
        recall_sum += num_examples * metrics["recall"]
        f1_sum += num_examples * metrics["f1"]
        total_examples += num_examples

    # Aggregate metrics
    return {
        "accuracy": accuracy_sum / total_examples,
        "precision": precision_sum / total_examples,
        "recall": recall_sum / total_examples,
        "f1": f1_sum / total_examples,
    }


# Neural Network

# In[ ]:


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from collections import OrderedDict

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# In[ ]:


# Define Training Function
def train(model, trainloader, epochs):
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    for _ in range(epochs):
        for X, y in trainloader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()


# In[ ]:


def test(model, testloader):
    criterion = nn.BCELoss()
    correct, total, loss = 0, 0, 0
    predicted_labels = []
    true_labels = []

    with torch.no_grad():
        for X, y in testloader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            output = model(X)
            loss += criterion(output, y).item()
            pred = torch.round(output)
            correct += (pred == y).sum().item()
            total += y.size(0)

            predicted_labels.append(pred.cpu().numpy())
            true_labels.append(y.cpu().numpy())

    accuracy = correct / total
    return loss, accuracy, predicted_labels, true_labels


# In[ ]:


def testadv(model, testloaderadv):
    criterion = nn.BCELoss()
    correct, total, loss = 0, 0, 0
    predicted_labels = []
    true_labels = []

    with torch.no_grad():
        for X, y in testloaderadv:
            X, y = X.to(DEVICE), y.to(DEVICE)
            output = model(X)
            loss += criterion(output, y).item()
            pred = torch.round(output)
            correct += (pred == y).sum().item()
            total += y.size(0)

            predicted_labels.append(pred.cpu().numpy())
            true_labels.append(y.cpu().numpy())

    accuracy = correct / total
    return loss, accuracy, predicted_labels, true_labels


# In[ ]:


# Define Data Class
class Data(Dataset):
    def __init__(self, features, targets):
        self.features = torch.Tensor(features)
        self.targets = torch.Tensor(targets)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]


# Load Dataset

# In[ ]:


BATCH_SIZE = 32

# Load data for each client


def load_datasets(batch_size=32, num_workers=0, val_size=0.1, random_seed=42):
    trainloaders = []
    valloaders = []
    testloaders = []

    for train_set in range(len(X_train_store)):
        X_train = X_train_store[train_set]
        y_train = y_train_store[train_set]

        # Split train set into train and validation sets
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.1, random_state=43
        )

        # Convert the data into tensors
        X_train_tensor = torch.Tensor(X_train)
        y_train_tensor = torch.Tensor(np.array(y_train)).unsqueeze(1)
        X_val_tensor = torch.Tensor(X_val)
        y_val_tensor = torch.Tensor(np.array(y_val)).unsqueeze(1)
        X_test_tensor = torch.Tensor(X_test_store[train_set])
        y_test_tensor = torch.Tensor(np.array(y_test_store[train_set])).unsqueeze(1)

        # Create train, validation, and test datasets
        train_dataset = Data(X_train_tensor, y_train_tensor)
        val_dataset = Data(X_val_tensor, y_val_tensor)
        test_dataset = Data(X_test_tensor, y_test_tensor)

        # Create train, validation
        trainloaders.append(
            DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        )
        valloaders.append(DataLoader(val_dataset, batch_size=batch_size, shuffle=False))
        testloaders.append(
            DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        )

    return trainloaders, valloaders, testloaders


trainloaders, valloaders, testloaders = load_datasets()


# In[ ]:


BATCH_SIZE = 32

# Load data for each client


def loadadv_datasets(batch_size=32, num_workers=0, val_size=0.1, random_seed=42):
    trainloadersadv = []
    valloadersadv = []
    testloadersadv = []

    for train_set in range(len(Xadv_train_store)):
        Xadv_train = Xadv_train_store[train_set]
        yadv_train = yadv_train_store[train_set]

        # Split train set into train and validation sets
        Xadv_train, Xadv_val, yadv_train, yadv_val = train_test_split(
            Xadv_train, yadv_train, test_size=0.1, random_state=43
        )

        # Convert the data into tensors
        Xadv_train_tensor = torch.Tensor(Xadv_train)
        yadv_train_tensor = torch.Tensor(np.array(yadv_train)).unsqueeze(1)
        Xadv_val_tensor = torch.Tensor(Xadv_val)
        yadv_val_tensor = torch.Tensor(np.array(yadv_val)).unsqueeze(1)
        Xadv_test_tensor = torch.Tensor(Xadv_test_store[train_set])
        yadv_test_tensor = torch.Tensor(np.array(yadv_test_store[train_set])).unsqueeze(1)

        # Create train, validation, and test datasets
        trainadv_dataset = Data(Xadv_train_tensor, yadv_train_tensor)
        valadv_dataset = Data(Xadv_val_tensor, yadv_val_tensor)
        testadv_dataset = Data(Xadv_test_tensor, yadv_test_tensor)

        # Create train, validation
        trainloadersadv.append(
            DataLoader(trainadv_dataset, batch_size=batch_size, shuffle=True)
        )
        valloadersadv.append(DataLoader(valadv_dataset, batch_size=batch_size, shuffle=False))
        testloadersadv.append(
            DataLoader(testadv_dataset, batch_size=batch_size, shuffle=False)
        )

    return trainloadersadv, valloadersadv, testloadersadv


trainloadersadv, valloadersadv, testloadersadv = loadadv_datasets()


# In[ ]:


# Function to get the parameters of the model
def get_parameters(net):
    return [val.cpu().numpy() for _, val in net.state_dict().items()]


# In[ ]:


# Set parameters
def set_parameters(net, parameters):
    params_dict = zip(net.state_dict().keys(), parameters)
    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
    net.load_state_dict(state_dict, strict=True)


# Define Model

# In[ ]:


# Define a Feedforward Neural Network
class Model(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return torch.sigmoid(self.fc2(x))


# Make a Flower Client Class

# In[ ]:


# Define the client_fn function
def client_fn(id, round_num):
    # Convert id to int
    id = int(id)

    # Create train, validation, and test datasets
    model = Model(input_size=X_train.shape[1], hidden_size=32).to(DEVICE)

    # Create train, validation, and test datasets based on the round number
    if round_num == 3:
        testloader = testloadersadv[id]  # Use adversarial test data in round 3
    else:
        testloader = testloaders[id]  # Use original test data for other rounds

    return NNClient(model, trainloaders[id], valloaders[id], testloader, testloadersadv[id])


from sklearn.metrics import precision_recall_fscore_support

class NNClient(fl.client.NumPyClient):
    def __init__(self, net, trainloader, valloader, testloader, testloaderadv):
        self.net = net
        self.trainloader = trainloader
        self.valloader = valloader
        self.testloader = testloader
        self.testloaderadv = testloaderadv
        self.val_predicted_labels = []
        self.val_true_labels = []
        self.test_predicted_labels = []
        self.test_true_labels = []

    def get_parameters(self, config):
        return get_parameters(self.net)

    def fit(self, parameters, config):
        set_parameters(self.net, parameters)
        train(self.net, self.trainloader, epochs=1)
        return get_parameters(self.net), len(self.trainloader), {}

    def evaluate(self, parameters, config):
        set_parameters(self.net, parameters)
        loss, accuracy, predicted_labels, true_labels = test(self.net, self.valloader)

        self.val_predicted_labels.append(predicted_labels)
        self.val_true_labels.append(true_labels)

        # Calculate precision, recall, and F1 score for the current validation round
        precision, recall, f1, _ = precision_recall_fscore_support(
            np.concatenate(true_labels),
            np.concatenate(predicted_labels),
            average='binary'  
        )

        # Return the additional metrics along with loss and accuracy
        return float(loss), len(self.valloader), {
            "accuracy": float(accuracy),
            "precision": precision,
            "recall": recall,
            "f1": f1
        }


# In[ ]:


from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

class NNClient(fl.client.NumPyClient):
    def __init__(self, net, trainloader, trainloaderadv, valloader):
        self.net = net
        self.trainloader = trainloader
        #self.trainloaderadv = trainloaderadv
        self.valloader = valloader
        self.predicted_labels = []
        self.true_labels = []

    def get_parameters(self, config):
        return get_parameters(self.net)

    def fit(self, parameters, config, round_num):
        if round_num <= 5:
            set_parameters(self.net, parameters)
            train(self.net, self.trainloader, epochs=1)
            return get_parameters(self.net), len(self.trainloader), {}
        else:
            set_parameters(self.net, parameters)
            train(self.net, self.trainloaderadv, epochs=1)
            return get_parameters(self.net), len(self.trainloaderadv), {}

    def calculate_accuracy(self, true_labels, predicted_labels):
        true_flat = np.concatenate(true_labels)
        predicted_flat = np.concatenate(predicted_labels)
        return accuracy_score(true_flat, predicted_flat)

    def evaluate(self, parameters, config):
        set_parameters(self.net, parameters)
        loss, accuracy, predicted_labels, true_labels = test(self.net, self.valloader)

        self.predicted_labels.append(predicted_labels)
        self.true_labels.append(true_labels)

        if len(self.true_labels) >= 2:
            # Calculate accuracy for the last two rounds (original vs. perturbed)
            accuracy_original = accuracy_score(
                np.concatenate(self.true_labels[-2]),
                np.concatenate(self.predicted_labels[-2])
            )
            accuracy_perturbed = accuracy_score(
                np.concatenate(self.true_labels[-1]),
                np.concatenate(self.predicted_labels[-1])
            )
            accuracy_drop = accuracy_original - accuracy_perturbed

        # Calculate precision, recall, and F1 score for the current round
        precision, recall, f1, _ = precision_recall_fscore_support(
            np.concatenate(true_labels),
            np.concatenate(predicted_labels),
            average='binary'  # Change this to 'micro', 'macro', etc. as needed
        )

        # Return the additional metrics along with loss and accuracy
        return float(loss), len(self.valloader), {
            "accuracy": float(accuracy),
            "precision": precision,
            "recall": recall,
            "f1": f1
        }


# Instantiate the Client

# In[ ]:


def client_fn(id, round_num):
    id = int(id)
    model = Model(input_size=X_train.shape[1], hidden_size=32).to(DEVICE)
    trainloader = trainloaders[id]
    trainloaderadv = trainloadersadv[id]
    valloader = valloaders[id]
    return NNClient(model, trainloader, trainloaderadv, valloader)


# Strategy

# In[ ]:


# Define strategy
strategy = fl.server.strategy.FedAvg(
    fraction_fit=1.0,  # the fraction of available clients used for training at each round
    fraction_evaluate=0.5,  # the fraction of available clients used for evaluation at each round
    min_fit_clients=10,  # minimum number of clients used for training at each round
    min_evaluate_clients=5,  # minimum number of clients used for evaluation at each round
    min_available_clients=10,  # minimum number of all available clients to be considered
    evaluate_metrics_aggregation_fn=weighted_average,  # function used for weighted model aggregation
)

# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)
client_resources = None
if DEVICE.type == "cuda":
    client_resources = {"num_gpus": 1}


# Simulated Training

# In[ ]:


# Start Simulated Training
fnn_scores = fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=num_clients,
    config=fl.server.ServerConfig(num_rounds=5),
    strategy=strategy,
    client_resources=client_resources,
)

# Extract dict from History object
fnn_scores = fnn_scores.__dict__

# print the final score after training
print(f"\nFedAvg final score: {fnn_scores['metrics_distributed']['accuracy'][-1][1]}")
print(f"FedAvg final loss: {fnn_scores['losses_distributed'][-1][1]}")


# In[ ]:


num_rounds = 5  # define the num_rounds variable

# Inside the simulation loop
for round_num in range(1, num_rounds + 1):
    fnn_scores = fl.simulation.start_simulation(
        client_fn=lambda cid: client_fn(cid, round_num),  # Pass round_num to client_fn
        num_clients=num_clients,
        config=fl.server.ServerConfig(num_rounds=5),  # Corrected the comma here
        strategy=strategy,
        client_resources=client_resources,
    )

    # Extract dict from History object
    fnn_scores = fnn_scores.__dict__

    # Print the final score after training for this round
    print(f"\nRound {round_num} - FedAvg final score: {fnn_scores['metrics_distributed']['accuracy'][-1][1]}")
    print(f"Round {round_num} - FedAvg final loss: {fnn_scores['losses_distributed'][-1][1]}")


# In[ ]:


import matplotlib.pyplot as plt
from tabulate import tabulate

# Extract metrics from the 'metrics_distributed' dictionary (assuming this structure)
metrics_distributed = fnn_scores['metrics_distributed']

# Prepare data for tabulation
metrics_data = []
for round_num, values in enumerate(metrics_distributed['accuracy']):
    metrics_data.append({
        'Round': round_num + 1,
        'Accuracy': values[1],
        'Precision': metrics_distributed['precision'][round_num][1],
        'Recall': metrics_distributed['recall'][round_num][1],
        'F1 Score': metrics_distributed['f1'][round_num][1]
    })

# Print tabulated metrics
table = tabulate(metrics_data, headers="keys", tablefmt="grid")
print(table)


# In[ ]:


# Extract metrics from the 'metrics_distributed' dictionary
rounds_float = [entry[0] for entry in fnn_scores['metrics_distributed']['accuracy']]
rounds = [int(round(round_num)) for round_num in rounds_float]  # Convert float rounds to integers

accuracy_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['accuracy']]
precision_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['precision']]
recall_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['recall']]
f1_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['f1']]

# Plot metrics
plt.figure(figsize=(10, 6))
plt.plot(rounds, accuracy_scores, label='Accuracy')
plt.plot(rounds, precision_scores, label='Precision')
plt.plot(rounds, recall_scores, label='Recall')
plt.plot(rounds, f1_scores, label='F1 Score')
plt.xticks(rounds)  # Set x-axis ticks to integer round numbers
plt.xlabel('Round')
plt.ylabel('Metric Value')
plt.title('Metrics Over Rounds')
plt.legend()
plt.grid(True)
plt.show()


# In[ ]:


print(fnn_scores)


# In[ ]:


import pprint

pprint.pprint(fnn_scores)


# Plot Results

# In[ ]:


# Get Accuracy
acc = [i[1] for i in fnn_scores["metrics_distributed"]["accuracy"]]

# Plot Accuracy per Round
fig = px.line(x=range(len(acc)), y=acc, title="FedAvg Accuracy per Round")
# x step size 1 and title
fig.update_xaxes(dtick=1, title_text="Round")
# y title
fig.update_yaxes(title_text="Accuracy")
# green color line
fig.update_traces(line_color="green")
fig.show(config={"displayModeBar": False})


# In[ ]:


# Get Loss
loss = [i[1] for i in fnn_scores["losses_distributed"]]

# Plot Accuracy per Round
fig = px.line(x=range(len(loss)), y=loss, title="FedAvg Loss per Round")
# x step size 1 and title
fig.update_xaxes(dtick=1, title_text="Round")
# y title
fig.update_yaxes(title_text="Loss")
# green color line
fig.update_traces(line_color="red")
fig.show(config={"displayModeBar": False})


# Testing with Adversarial samples from 4 to 6th rounds of simulation

# In[ ]:


# Define the client_fn function
def client_fn(id, round_num):
    # Convert id to int
    id = int(id)

    # Create train, validation, and test datasets
    model = Model(input_size=X_train.shape[1], hidden_size=32).to(DEVICE)

    # Create train, validation, and test datasets based on the round number
    if 4 <= round_num <= 6:
        testloader = testloadersadv[id]  # Use adversarial test data in round 3
    else:
        testloader = testloaders[id]  # Use original test data for other rounds

    return NNClient(model, trainloaders[id], valloaders[id], testloader, testloadersadv[id])
    #return NNClient(model, trainloaders[id], valloaders[id], testloader, testloadersadv[id])


from sklearn.metrics import precision_recall_fscore_support

class NNClient(fl.client.NumPyClient):
    def __init__(self, net, trainloader, valloader, testloader, testloaderadv):
        self.net = net
        self.trainloader = trainloader
        self.valloader = valloader
        self.testloader = testloader
        self.testloaderadv = testloaderadv
        self.val_predicted_labels = []
        self.val_true_labels = []
        self.test_predicted_labels = []
        self.test_true_labels = []

    def get_parameters(self, config):
        return get_parameters(self.net)

    def fit(self, parameters, config):
        set_parameters(self.net, parameters)
        train(self.net, self.trainloader, epochs=1)
        return get_parameters(self.net), len(self.trainloader), {}

    def evaluate(self, parameters, config):
        set_parameters(self.net, parameters)
        loss, accuracy, predicted_labels, true_labels = test(self.net, self.valloader)

        self.val_predicted_labels.append(predicted_labels)
        self.val_true_labels.append(true_labels)

        # Calculate precision, recall, and F1 score for the current validation round
        precision, recall, f1, _ = precision_recall_fscore_support(
            np.concatenate(true_labels),
            np.concatenate(predicted_labels),
            average='binary'  # Change this to 'micro', 'macro', etc. as needed
        )

        # Return the additional metrics along with loss and accuracy
        return float(loss), len(self.valloader), {
            "accuracy": float(accuracy),
            "precision": precision,
            "recall": recall,
            "f1": f1
        }


# In[ ]:


# Define strategy
strategy = fl.server.strategy.FedAvg(
    fraction_fit=1.0,  # the fraction of available clients used for training at each round
    fraction_evaluate=0.5,  # the fraction of available clients used for evaluation at each round
    min_fit_clients=10,  # minimum number of clients used for training at each round
    min_evaluate_clients=5,  # minimum number of clients used for evaluation at each round
    min_available_clients=10,  # minimum number of all available clients to be considered
    evaluate_metrics_aggregation_fn=weighted_average,  # function used for weighted model aggregation
)

# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)
client_resources = None
if DEVICE.type == "cuda":
    client_resources = {"num_gpus": 1}


num_rounds = 10  # Assuming you have defined the num_rounds variable

# Inside the simulation loop
for round_num in range(1, num_rounds + 1):
    fnn_scores = fl.simulation.start_simulation(
        client_fn=lambda cid: client_fn(cid, round_num),  # Pass round_num to client_fn
        num_clients=num_clients,
        config=fl.server.ServerConfig(num_rounds=10),  # Corrected the comma here
        strategy=strategy,
        client_resources=client_resources,
    )

    # Extract dict from History object
    fnn_scores = fnn_scores.__dict__

    # Print the final score after training for this round
    print(f"\nRound {round_num} - FedAvg final score: {fnn_scores['metrics_distributed']['accuracy'][-1][1]}")
    print(f"Round {round_num} - FedAvg final loss: {fnn_scores['losses_distributed'][-1][1]}")


# In[ ]:


import matplotlib.pyplot as plt
from tabulate import tabulate

# Extract metrics from the 'metrics_distributed' dictionary (assuming this structure)
metrics_distributed = fnn_scores['metrics_distributed']

# Prepare data for tabulation
metrics_data = []
for round_num, values in enumerate(metrics_distributed['accuracy']):
    metrics_data.append({
        'Round': round_num + 1,
        'Accuracy': values[1],
        'Precision': metrics_distributed['precision'][round_num][1],
        'Recall': metrics_distributed['recall'][round_num][1],
        'F1 Score': metrics_distributed['f1'][round_num][1]
    })

# Print tabulated metrics
table = tabulate(metrics_data, headers="keys", tablefmt="grid")
print(table)

# Extract metrics from the 'metrics_distributed' dictionary
rounds_float = [entry[0] for entry in fnn_scores['metrics_distributed']['accuracy']]
rounds = [int(round(round_num)) for round_num in rounds_float]  # Convert float rounds to integers

accuracy_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['accuracy']]
precision_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['precision']]
recall_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['recall']]
f1_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['f1']]

# Plot metrics
plt.figure(figsize=(10, 6))
plt.plot(rounds, accuracy_scores, label='Accuracy')
plt.plot(rounds, precision_scores, label='Precision')
plt.plot(rounds, recall_scores, label='Recall')
plt.plot(rounds, f1_scores, label='F1 Score')
plt.xticks(rounds)  # Set x-axis ticks to integer round numbers
plt.xlabel('Round')
plt.ylabel('Metric Value')
plt.title('Metrics Over Rounds')
plt.legend()
plt.grid(True)
plt.show()


# Get Accuracy
acc = [i[1] for i in fnn_scores["metrics_distributed"]["accuracy"]]

# Plot Accuracy per Round
fig = px.line(x=range(len(acc)), y=acc, title="FedAvg Accuracy per Round")
# x step size 1 and title
fig.update_xaxes(dtick=1, title_text="Round")
# y title
fig.update_yaxes(title_text="Accuracy")
# green color line
fig.update_traces(line_color="green")
fig.show(config={"displayModeBar": False})

# Get Loss
loss = [i[1] for i in fnn_scores["losses_distributed"]]

# Plot Accuracy per Round
fig = px.line(x=range(len(loss)), y=loss, title="FedAvg Loss per Round")
# x step size 1 and title
fig.update_xaxes(dtick=1, title_text="Round")
# y title
fig.update_yaxes(title_text="Loss")
# green color line
fig.update_traces(line_color="red")
fig.show(config={"displayModeBar": False})


# Testing with Adversarial samples from 6th, 8th, 12th, 14th rounds of simulation

# In[ ]:


# Define the client_fn function
def client_fn(id, round_num):
    # Convert id to int
    id = int(id)

    # Create train, validation, and test datasets
    model = Model(input_size=X_train.shape[1], hidden_size=32).to(DEVICE)

     # Create train, validation, and test datasets based on the round number
    if round_num in [6, 8, 12, 14]:
        testloader = testloadersadv[id]  # Use adversarial test data in round 3
    else:
        testloader = testloaders[id]  # Use original test data for other rounds

    return NNClient(model, trainloaders[id], valloaders[id], testloader, testloadersadv[id])
    #return NNClient(model, trainloaders[id], valloaders[id], testloader, testloadersadv[id])


from sklearn.metrics import precision_recall_fscore_support

class NNClient(fl.client.NumPyClient):
    def __init__(self, net, trainloader, valloader, testloader, testloaderadv):
        self.net = net
        self.trainloader = trainloader
        self.valloader = valloader
        self.testloader = testloader
        self.testloaderadv = testloaderadv
        self.val_predicted_labels = []
        self.val_true_labels = []
        self.test_predicted_labels = []
        self.test_true_labels = []

    def get_parameters(self, config):
        return get_parameters(self.net)

    def fit(self, parameters, config):
        set_parameters(self.net, parameters)
        train(self.net, self.trainloader, epochs=1)
        return get_parameters(self.net), len(self.trainloader), {}

    def evaluate(self, parameters, config):
        set_parameters(self.net, parameters)
        loss, accuracy, predicted_labels, true_labels = test(self.net, self.valloader)

        self.val_predicted_labels.append(predicted_labels)
        self.val_true_labels.append(true_labels)

        # Calculate precision, recall, and F1 score for the current validation round
        precision, recall, f1, _ = precision_recall_fscore_support(
            np.concatenate(true_labels),
            np.concatenate(predicted_labels),
            average='binary'  # Change this to 'micro', 'macro', etc. as needed
        )

        # Return the additional metrics along with loss and accuracy
        return float(loss), len(self.valloader), {
            "accuracy": float(accuracy),
            "precision": precision,
            "recall": recall,
            "f1": f1
        }




# Define strategy
strategy = fl.server.strategy.FedAvg(
    fraction_fit=1.0,  # the fraction of available clients used for training at each round
    fraction_evaluate=0.5,  # the fraction of available clients used for evaluation at each round
    min_fit_clients=10,  # minimum number of clients used for training at each round
    min_evaluate_clients=5,  # minimum number of clients used for evaluation at each round
    min_available_clients=10,  # minimum number of all available clients to be considered
    evaluate_metrics_aggregation_fn=weighted_average,  # function used for weighted model aggregation
)

# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)
client_resources = None
if DEVICE.type == "cuda":
    client_resources = {"num_gpus": 1}


num_rounds = 20  # Assuming you have defined the num_rounds variable

# Inside the simulation loop
for round_num in range(1, num_rounds + 1):
    fnn_scores = fl.simulation.start_simulation(
        client_fn=lambda cid: client_fn(cid, round_num),  # Pass round_num to client_fn
        num_clients=num_clients,
        config=fl.server.ServerConfig(num_rounds=20),  # Corrected the comma here
        strategy=strategy,
        client_resources=client_resources,
    )

    # Extract dict from History object
    fnn_scores = fnn_scores.__dict__

    # Print the final score after training for this round
    print(f"\nRound {round_num} - FedAvg final score: {fnn_scores['metrics_distributed']['accuracy'][-1][1]}")
    print(f"Round {round_num} - FedAvg final loss: {fnn_scores['losses_distributed'][-1][1]}")


#Plot

import matplotlib.pyplot as plt
from tabulate import tabulate

# Extract metrics from the 'metrics_distributed' dictionary 
metrics_distributed = fnn_scores['metrics_distributed']

# Prepare data for tabulation
metrics_data = []
for round_num, values in enumerate(metrics_distributed['accuracy']):
    metrics_data.append({
        'Round': round_num + 1,
        'Accuracy': values[1],
        'Precision': metrics_distributed['precision'][round_num][1],
        'Recall': metrics_distributed['recall'][round_num][1],
        'F1 Score': metrics_distributed['f1'][round_num][1]
    })

# Print tabulated metrics
table = tabulate(metrics_data, headers="keys", tablefmt="grid")
print(table)

# Extract metrics from the 'metrics_distributed' dictionary
rounds_float = [entry[0] for entry in fnn_scores['metrics_distributed']['accuracy']]
rounds = [int(round(round_num)) for round_num in rounds_float]  # Convert float rounds to integers

accuracy_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['accuracy']]
precision_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['precision']]
recall_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['recall']]
f1_scores = [entry[1] for entry in fnn_scores['metrics_distributed']['f1']]

# Plot metrics
plt.figure(figsize=(10, 6))
plt.plot(rounds, accuracy_scores, label='Accuracy')
plt.plot(rounds, precision_scores, label='Precision')
plt.plot(rounds, recall_scores, label='Recall')
plt.plot(rounds, f1_scores, label='F1 Score')
plt.xticks(rounds)  # Set x-axis ticks to integer round numbers
plt.xlabel('Round')
plt.ylabel('Metric Value')
plt.title('Metrics Over Rounds')
plt.legend()
plt.grid(True)
plt.show()


# Get Accuracy
acc = [i[1] for i in fnn_scores["metrics_distributed"]["accuracy"]]

# Plot Accuracy per Round
fig = px.line(x=range(len(acc)), y=acc, title="FedAvg Accuracy per Round")
# x step size 1 and title
fig.update_xaxes(dtick=1, title_text="Round")
# y title
fig.update_yaxes(title_text="Accuracy")
# green color line
fig.update_traces(line_color="green")
fig.show(config={"displayModeBar": False})

# Get Loss
loss = [i[1] for i in fnn_scores["losses_distributed"]]

# Plot Accuracy per Round
fig = px.line(x=range(len(loss)), y=loss, title="FedAvg Loss per Round")
# x step size 1 and title
fig.update_xaxes(dtick=1, title_text="Round")
# y title
fig.update_yaxes(title_text="Loss")
# green color line
fig.update_traces(line_color="red")
fig.show(config={"displayModeBar": False})


# In[ ]:




    # Print the final score after training for this round
print(f"\nRound {round_num} - FedAvg final score: {fnn_scores['metrics_distributed']['accuracy'][-1][1]}")
print(f"Round {round_num} - FedAvg final loss: {fnn_scores['losses_distributed'][-1][1]}")

